# Cell 1: Import libraries and setup
import pandas as pd
import polars as pl
import chromadb
import chromadb.utils.embedding_functions as embedding_functions
from pprint import pprint
import json
import re

print("Libraries imported successfully!")

# Cell 2: Read CS programs CSV data
# Using pandas for better JSON handling, then convert to polars if needed
df = pd.read_csv("global_cs_programs.csv").fillna('')

print(f"DataFrame shape: {df.shape}")
print("\nColumn names:")
pprint(list(df.columns))
print("\nFirst 2 rows preview:")
print(df[['program_name', 'university', 'region', 'tier']].head(2))

# Cell 3: Initialize ChromaDB with persistent storage
chroma_client = chromadb.PersistentClient(path="vectors")

# Initialize Google Gemini embedding function
google_ef = embedding_functions.GoogleGenerativeAiEmbeddingFunction(
    api_key="API_KEY",
    model_name="gemini-embedding-001"
)

print("ChromaDB and embedding function initialized!")

# Cell 4: Data preprocessing and document creation
def process_admission_data(admission_data_str):
    """Convert admission_data string to readable text"""
    if not admission_data_str or admission_data_str == 'null':
        return "æš‚æ— å½•å–æ¡ˆä¾‹æ•°æ®"
    
    try:
        # Handle string format list conversion
        data_str = admission_data_str.replace("'", "\"")
        data_list = json.loads(data_str)
        
        cases = []
        for case in data_list:
            case_text = f"å½•å–æ¡ˆä¾‹({case.get('å½•å–æ—¶é—´', 'N/A')}): {case.get('å½•å–ç»“æœ', 'N/A')}ã€‚"
            case_text += f"ç”³è¯·è€…èƒŒæ™¯: {case.get('å­¦æ ¡ï¼ˆæ¡£æ¬¡ï¼‰', 'N/A')} {case.get('æœ¬ç§‘ä¸“ä¸š', 'N/A')}ä¸“ä¸šï¼Œ"
            case_text += f"GPA/Rank {case.get('GPA/Rank', 'N/A')}ï¼Œ"
            case_text += f"ç§‘ç ”ç»å† {case.get('ç§‘ç ”ç»å†', 'N/A')}ï¼Œ"
            case_text += f"å®ä¹ ç»å† {case.get('å®ä¹ ç»å†', 'N/A')}ã€‚"
            if case.get('å…¶ä»–ï¼ˆè¯­è¨€/æ¨èä¿¡ï¼‰'):
                case_text += f"å…¶ä»–ä¿¡æ¯: {case.get('å…¶ä»–ï¼ˆè¯­è¨€/æ¨èä¿¡ï¼‰', '')}"
            cases.append(case_text)
        
        return " ".join(cases)
    except (json.JSONDecodeError, TypeError, AttributeError):
        return f"å½•å–æ•°æ®: {admission_data_str}"

def create_document_text(row):
    """Create rich document text for semantic search"""
    admission_text = process_admission_data(row['admission_data'])
    
    document = f"""
é¡¹ç›®åç§°: {row['program_name']}
æ‰€å±å¤§å­¦: {row['university']}
åœ°åŒº: {row['region']}
é¡¹ç›®ç­‰çº§: {row['tier']}
å­¦åˆ¶: {row['duration']}
æˆè¯¾è¯­è¨€: {row['language']}
å­¦ä½ç±»å‹: {row['degree_type']}

é¡¹ç›®ä¼˜ç‚¹: {row['pros']}

é¡¹ç›®ç¼ºç‚¹: {row['cons']}

æ‹›ç”Ÿåå¥½: {row['admission_preference']}

ç”³è¯·æ³¨æ„äº‹é¡¹: {row['application_notes']}

å¥–å­¦é‡‘ä¿¡æ¯: {row['scholarship']}

è¿‡å¾€å½•å–æ¡ˆä¾‹: {admission_text}

å…¶ä»–ä¿¡æ¯: {row['other_info']} {row['other_notes']}
    """
    return document.strip()

print("Data processing functions defined!")

# Cell 5: Process all data for ChromaDB
documents = []
metadatas = []
ids = []

print(f"Processing {len(df)} CS programs...")

for index, row in df.iterrows():
    # Create document text
    doc_text = create_document_text(row)
    documents.append(doc_text)
    
    # Create metadata for filtering
    metadata = {
        'program_name': str(row['program_name']),
        'university': str(row['university']),
        'region': str(row['region']),
        'tier': str(row['tier']),
        'duration': str(row['duration']),
        'language': str(row['language']),
        'degree_type': str(row['degree_type']),
        'internship_required': True if str(row['internship_required']).strip() == 'æ˜¯' else False,
        'thesis_required': True if str(row['thesis_required']).strip() == 'æ˜¯' else False,
        'admission_data_count': int(row['admission_data_count']) if str(row['admission_data_count']).isdigit() else 0
    }
    metadatas.append(metadata)
    
    # Create unique ID
    ids.append(f"program_{index}")

print(f"Processed {len(documents)} programs successfully!")

# Cell 6: Create ChromaDB collection
collection_name = "cs_programs_collection"

# Delete existing collection if it exists
try:
    chroma_client.delete_collection(name=collection_name)
    print(f"Deleted existing collection: {collection_name}")
except:
    print("No existing collection to delete")

# Create new collection with embedding function
collection = chroma_client.create_collection(
    name=collection_name,
    embedding_function=google_ef
)

print(f"Created collection: {collection_name}")

# Cell 7: Add data to collection in batches
batch_size = 10
total_batches = (len(documents) + batch_size - 1) // batch_size

print(f"Adding {len(documents)} programs in {total_batches} batches of {batch_size}")

for i in range(0, len(documents), batch_size):
    batch_docs = documents[i:i+batch_size]
    batch_ids = ids[i:i+batch_size]
    batch_metadata = metadatas[i:i+batch_size]
    
    batch_num = (i // batch_size) + 1
    
    print(f"Processing batch {batch_num}/{total_batches}")
    
    try:
        collection.add(
            documents=batch_docs,
            metadatas=batch_metadata,
            ids=batch_ids
        )
        print(f"âœ“ Added batch {batch_num}")
        
    except Exception as e:
        print(f"âœ— Error in batch {batch_num}: {str(e)}")
        break

print(f"\nCollection created with {collection.count()} programs")

# Cell 8: Test semantic search queries with filters
print("\n=== Testing Semantic Search with Filters ===")

# Query 1: Find TOP-TIER programs suitable for mainland Chinese students
results1 = collection.query(
    query_texts=["é™†æœ¬èƒŒæ™¯ç”³è¯· å½•å–æ¡ˆä¾‹ å‹å¥½"],
    n_results=5,
    where={"$and": [{"tier": {"$in": ["T0", "T1"]}}, {"admission_data_count": {"$gt": 0}}]}
)

print("\nğŸ” Query: 'T0/T1çº§åˆ«ä¸”æœ‰é™†æœ¬å½•å–æ¡ˆä¾‹çš„é¡¹ç›®'")
for i, (doc, metadata, distance) in enumerate(zip(
    results1['documents'][0], 
    results1['metadatas'][0], 
    results1['distances'][0]
)):
    print(f"\nğŸ“‹ Result {i+1} (ç›¸ä¼¼åº¦: {1-distance:.3f}):")
    print(f"é¡¹ç›®: {metadata['program_name']}")
    print(f"å¤§å­¦: {metadata['university']}")
    print(f"åœ°åŒº: {metadata['region']}")
    print(f"ç­‰çº§: {metadata['tier']}")
    print(f"å­¦åˆ¶: {metadata['duration']}")
    print(f"å½•å–æ¡ˆä¾‹æ•°: {metadata['admission_data_count']}")

# Cell 9: More targeted queries with duration filter
# Query 2: Short duration programs (filter for programs â‰¤ 12 months)
results2 = collection.query(
    query_texts=["å­¦åˆ¶çŸ­ å¿«é€Ÿæ¯•ä¸š æ—¶é—´ç´§å‡‘"],
    n_results=5,
    where={"duration": {"$in": ["9ä¸ªæœˆ", "10ä¸ªæœˆ", "11ä¸ªæœˆ", "12ä¸ªæœˆ", "1å¹´", "ä¸€å¹´"]}}
)

print("\nğŸ” Query: '1å¹´ä»¥å†…çš„çŸ­å­¦åˆ¶ä¼˜è´¨é¡¹ç›®'")
for i, (doc, metadata, distance) in enumerate(zip(
    results2['documents'][0], 
    results2['metadatas'][0], 
    results2['distances'][0]
)):
    print(f"\nğŸ“‹ Result {i+1}:")
    print(f"é¡¹ç›®: {metadata['program_name']}")
    print(f"å­¦åˆ¶: {metadata['duration']}")
    print(f"å¤§å­¦: {metadata['university']}")
    print(f"ç­‰çº§: {metadata['tier']}")
    print(f"åœ°åŒº: {metadata['region']}")

# Query 3: Scholarship opportunities (exclude programs with "è¾ƒéš¾è·å¾—" scholarships)
results3 = collection.query(
    query_texts=["å¥–å­¦é‡‘ èµ„åŠ© ç»æµæ”¯æŒ å®¹æ˜“ç”³è¯·"],
    n_results=5
)

print("\nğŸ” Query: 'æœ‰å¥–å­¦é‡‘æœºä¼šçš„ä¼˜è´¨é¡¹ç›®'")
for i, (doc, metadata, distance) in enumerate(zip(
    results3['documents'][0], 
    results3['metadatas'][0], 
    results3['distances'][0]
)):
    print(f"\nğŸ“‹ Result {i+1}:")
    print(f"é¡¹ç›®: {metadata['program_name']}")
    print(f"å¤§å­¦: {metadata['university']}")
    print(f"åœ°åŒº: {metadata['region']}")
    print(f"ç­‰çº§: {metadata['tier']}")

# Cell 10: Advanced filtering with metadata
print("\n=== Advanced Filtering ===")

# Filter by region and tier with better query
uk_t0_results = collection.query(
    query_texts=["è®¡ç®—æœºç§‘å­¦ äººå·¥æ™ºèƒ½ æœºå™¨å­¦ä¹  é¡¶çº§é¡¹ç›®"],
    n_results=5,
    where={"$and": [{"region": "è‹±å›½"}, {"tier": "T0"}]}
)

# Additional query: US programs with good admission data
us_programs = collection.query(
    query_texts=["è‹±å›½ å½•å–å‹å¥½ ç”³è¯·å»ºè®®"],
    n_results=3,
    where={"$and": [{"region": "è‹±å›½"}, {"admission_data_count": {"$gt": 0}}]}
)

print("\nğŸ¯ Filter: è‹±å›½T0çº§åˆ«é¡¹ç›®")
for i, (doc, metadata, distance) in enumerate(zip(
    uk_t0_results['documents'][0], 
    uk_t0_results['metadatas'][0], 
    uk_t0_results['distances'][0]
)):
    print(f"\nğŸ“‹ Result {i+1}:")
    print(f"é¡¹ç›®: {metadata['program_name']}")
    print(f"å¤§å­¦: {metadata['university']}")
    print(f"å­¦ä½ç±»å‹: {metadata['degree_type']}")
    print(f"æ˜¯å¦éœ€è¦è®ºæ–‡: {'æ˜¯' if metadata['thesis_required'] else 'å¦'}")

print("\nğŸ¯ Filter: è‹±å›½æœ‰å½•å–æ•°æ®çš„é¡¹ç›®")
for i, (doc, metadata, distance) in enumerate(zip(
    us_programs['documents'][0], 
    us_programs['metadatas'][0], 
    us_programs['distances'][0]
)):
    print(f"\nğŸ“‹ Result {i+1}:")
    print(f"é¡¹ç›®: {metadata['program_name']}")
    print(f"å¤§å­¦: {metadata['university']}")
    print(f"ç­‰çº§: {metadata['tier']}")
    print(f"å½•å–æ¡ˆä¾‹æ•°: {metadata['admission_data_count']}")

# Cell 11: Specialized queries for different needs
print("\n=== Specialized Queries ===")

# Query for programs without thesis requirement
no_thesis_programs = collection.query(
    query_texts=["ä¸éœ€è¦è®ºæ–‡ coursework æˆè¯¾å‹"],
    n_results=3,
    where={"$and": [{"thesis_required": False}, {"tier": {"$in": ["T0", "T1"]}}]}
)

print("\nğŸ¯ ä¸éœ€è¦è®ºæ–‡çš„é¡¶çº§é¡¹ç›®:")
for i, (doc, metadata, distance) in enumerate(zip(
    no_thesis_programs['documents'][0], 
    no_thesis_programs['metadatas'][0], 
    no_thesis_programs['distances'][0]
)):
    print(f"\nğŸ“‹ Result {i+1}:")
    print(f"é¡¹ç›®: {metadata['program_name']}")
    print(f"å¤§å­¦: {metadata['university']}")
    print(f"å­¦åˆ¶: {metadata['duration']}")
    print(f"ç­‰çº§: {metadata['tier']}")

# Query for English-taught programs in non-English countries
non_english_countries = collection.query(
    query_texts=["è‹±è¯­æˆè¯¾ å›½é™…é¡¹ç›®"],
    n_results=3,
    where={"$and": [
        {"language": "EN"}, 
        {"region": {"$nin": ["è‹±å›½", "æ¾³æ´²", "åŠ æ‹¿å¤§"]}}
    ]}
)

print("\nğŸ¯ éè‹±è¯­å›½å®¶çš„è‹±è¯­æˆè¯¾é¡¹ç›®:")
for i, (doc, metadata, distance) in enumerate(zip(
    non_english_countries['documents'][0], 
    non_english_countries['metadatas'][0], 
    non_english_countries['distances'][0]
)):
    print(f"\nğŸ“‹ Result {i+1}:")
    print(f"é¡¹ç›®: {metadata['program_name']}")
    print(f"å¤§å­¦: {metadata['university']}")
    print(f"åœ°åŒº: {metadata['region']}")
    print(f"è¯­è¨€: {metadata['language']}")

# Cell 12: Collection statistics and analysis
print("\n=== Collection Statistics ===")
print(f"æ€»é¡¹ç›®æ•°: {collection.count()}")

# Get all data for analysis
all_data = collection.get()

# Analyze by region
regions = [meta['region'] for meta in all_data['metadatas']]
region_counts = {}
for region in regions:
    region_counts[region] = region_counts.get(region, 0) + 1

print("\nğŸ“Š æŒ‰åœ°åŒºåˆ†å¸ƒ:")
for region, count in sorted(region_counts.items()):
    print(f"  {region}: {count} ä¸ªé¡¹ç›®")

# Analyze by tier
tiers = [meta['tier'] for meta in all_data['metadatas']]
tier_counts = {}
for tier in tiers:
    tier_counts[tier] = tier_counts.get(tier, 0) + 1

print("\nğŸ“Š æŒ‰ç­‰çº§åˆ†å¸ƒ:")
for tier, count in sorted(tier_counts.items()):
    print(f"  {tier}: {count} ä¸ªé¡¹ç›®")

# Analyze thesis requirements
thesis_required = sum(1 for meta in all_data['metadatas'] if meta['thesis_required'])
print(f"\nğŸ“Š éœ€è¦è®ºæ–‡çš„é¡¹ç›®: {thesis_required}/{len(all_data['metadatas'])}")

print("\nâœ… CS Programs ChromaDB setup complete!")
